# 测试标记功能使用指南

## 🏷️ 功能简介

测试标记功能允许你在运行测试时添加自定义标签，用于区分不同版本、环境或优化阶段的测试结果。标记会显示在任务名称中，方便后续对比分析。

## 📝 使用场景

### 1. **性能优化对比**
```
[优化前] B站首页 - 初始化性能测试
[优化后] B站首页 - 初始化性能测试
```
对比优化前后的性能差异

### 2. **版本对比**
```
[Baseline] 视频播放页 - Runtime性能监控
[V2.0] 视频播放页 - Runtime性能监控
[V2.1] 视频播放页 - Runtime性能监控
```
追踪不同版本的性能演进

### 3. **分支对比**
```
[主分支] 搜索页 - 综合性能测试
[功能分支-新搜索] 搜索页 - 综合性能测试
```
对比不同代码分支的性能

### 4. **环境对比**
```
[生产环境] 直播页 - 内存泄漏检测
[测试环境] 直播页 - 内存泄漏检测
[预发环境] 直播页 - 内存泄漏检测
```
对比不同环境的表现

### 5. **A/B测试**
```
[方案A-旧架构] 动态页 - 性能基准测试
[方案B-新架构] 动态页 - 性能基准测试
```
对比不同技术方案

## 🚀 使用方法

### 单个用例运行

1. **点击用例的"▶ 运行"按钮**
2. **输入测试标记**（弹出提示框）
   ```
   请输入本次测试的标记（可选）：

   建议标记格式：
   • 优化前 / 优化后
   • Baseline / V2.0
   • 主分支 / 功能分支

   标记将显示在任务名称中，留空则不添加标记
   ```
3. **确认执行**

### 批量并行运行

1. **勾选多个测试用例**
2. **点击"⚡ 并行运行"按钮**
3. **输入测试标记**（同样的提示框）
4. **查看确认信息**
   ```
   即将并行执行 5 个测试用例
   测试标记: 优化后
   服务器并发限制: 最多同时运行 10 个任务
   确认执行？
   ```
5. **确认执行**

## 📊 效果展示

### 任务列表显示
```
✅ [优化前] B站首页 - 初始化性能测试    (Initialization)
✅ [优化后] B站首页 - 初始化性能测试    (Initialization)
🏃 [Baseline] 视频播放页 - Runtime性能   (Runtime)
⏳ [V2.0] 视频播放页 - Runtime性能        (Runtime)
```

### 日志输出
```
========== 批量并行执行开始 ==========
[系统] 提交 3 个测试用例
[系统] 测试标记: 优化后
[系统] 服务器并发限制: 10 个
[1/3] 提交任务: [优化后] B站首页 - 初始化性能测试
[2/3] 提交任务: [优化后] 视频播放页 - Runtime性能监控
[3/3] 提交任务: [优化后] 动态页 - 内存泄漏检测
[系统] 批量提交完成，耗时 0.65秒
========== 批量并行执行结束 ==========
```

### 测试记录
测试标记会保存在测试记录中，可以在测试记录页面按标记筛选：
```
| 测试名称                                    | 标记    | 状态 | 耗时   |
|--------------------------------------------|---------|------|--------|
| [优化前] B站首页 - 初始化性能测试            | 优化前  | ✅   | 63.2s  |
| [优化后] B站首页 - 初始化性能测试            | 优化后  | ✅   | 48.5s  |
```

## 💡 最佳实践

### 1. **统一标记格式**
```
✅ 推荐：
  - 优化前 / 优化后
  - Baseline / V2.0 / V2.1
  - 主分支 / feature-xxx

❌ 避免：
  - 不一致的大小写：优化前 / OPTIMIZED
  - 随意缩写：opt-before / after_opt
  - 过长标记：这是优化后的版本使用了新的渲染引擎
```

### 2. **保持简短**
```
✅ 好的标记：
  - 优化前
  - V2.0
  - 测试环境

❌ 过长的标记：
  - 这是2025年1月20日优化后的版本
  - feature-refactor-video-player-v2
```

### 3. **使用有意义的标识**
```
✅ 清晰明确：
  - Chrome102 / Chrome110
  - 桌面端 / 移动端
  - 冷启动 / 热启动

❌ 模糊不清：
  - 测试1 / 测试2
  - 新版本 / 旧版本
```

### 4. **建立标记规范**
在团队中建立统一的标记规范文档，例如：
```markdown
# 性能测试标记规范

## 版本标记
- Baseline: 基准版本
- V{版本号}: 正式版本（如V2.0, V3.1）
- Dev-{日期}: 开发版本（如Dev-20250120）

## 环境标记
- Prod: 生产环境
- Staging: 预发环境
- Test: 测试环境

## 优化标记
- Before: 优化前
- After: 优化后
- Opt{序号}: 优化阶段（如Opt1, Opt2）
```

## 🔍 数据分析

### 对比相同用例不同标记的结果

1. **在测试记录页面筛选**
   - 按用例名称筛选
   - 查看不同标记的性能数据

2. **在可视化页面对比**
   - 选择相同的测试用例
   - 对比不同标记的图表

3. **导出数据分析**
   ```javascript
   // 筛选特定标记的测试记录
   const beforeTests = testRecords.filter(r => r.name.includes('[优化前]'));
   const afterTests = testRecords.filter(r => r.name.includes('[优化后]'));

   // 计算平均性能提升
   const avgBefore = average(beforeTests.map(t => t.duration));
   const avgAfter = average(afterTests.map(t => t.duration));
   const improvement = ((avgBefore - avgAfter) / avgBefore * 100).toFixed(2);

   console.log(`性能提升: ${improvement}%`);
   ```

## 🛠️ 技术实现

### 前端流程
```javascript
// 1. 用户点击运行
runCase(caseId) 或 runSelectedCases()

// 2. 弹出标记输入框
const testLabel = prompt('请输入本次测试的标记（可选）：', '');

// 3. 构建任务名称
const taskName = testLabel ? `[${testLabel}] ${testCase.name}` : testCase.name;

// 4. 发送到后端
fetch('/api/start', {
    method: 'POST',
    body: JSON.stringify({
        name: taskName,  // 带标记的名称
        config: { ... }
    })
});
```

### 后端存储
```typescript
// 任务对象包含完整的任务名称
interface Task {
    id: string;
    name: string;  // 例如: "[优化后] B站首页 - 初始化性能测试"
    runner: string;
    status: string;
    // ...
}

// 测试记录也会保存完整名称
interface TestRecord {
    id: string;
    name: string;  // 包含标记
    runner: string;
    status: string;
    // ...
}
```

### Perfcat集成
测试标记会显示在Perfcat报告URL中：
```
任务名称: [优化后] B站首页 - 初始化性能测试
Perfcat URL: https://fe-perfcat.bilibili.co/utils/shorten/xxx?runner=Initialization
```

## ❓ 常见问题

### Q: 可以不输入标记吗？
**A:** 可以，标记是可选的。留空或点击"取消"会使用原始的用例名称。

### Q: 标记可以包含特殊字符吗？
**A:** 建议只使用中文、英文、数字和简单符号（-、_、/），避免使用 `[]{}()` 等特殊字符。

### Q: 如何批量修改历史测试的标记？
**A:** 标记是在运行时添加的，无法修改历史记录。建议重新运行测试。

### Q: 标记会影响测试结果吗？
**A:** 不会，标记只是显示在名称中，不会影响实际的测试执行。

### Q: 可以自动填充上次的标记吗？
**A:** 当前版本暂不支持，每次都需要手动输入。可以通过复制粘贴快速填入。

## 📈 示例工作流

### 性能优化测试完整流程

```
1️⃣ 建立基准
   - 运行测试用例，标记为 "Baseline"
   - 记录性能数据

2️⃣ 进行优化
   - 修改代码、配置等

3️⃣ 优化后测试
   - 运行相同测试用例，标记为 "优化后"
   - 对比数据

4️⃣ 迭代优化
   - 运行测试，标记为 "优化V2"、"优化V3"
   - 持续对比

5️⃣ 最终验证
   - 运行测试，标记为 "Final"
   - 确认优化效果
```

### 实际操作示例
```bash
# 第1轮：优化前
勾选: [B站首页, 视频页, 动态页]
点击: ⚡ 并行运行
输入标记: "优化前"
✅ 完成，记录数据

# 进行代码优化...

# 第2轮：优化后
勾选: [B站首页, 视频页, 动态页]
点击: ⚡ 并行运行
输入标记: "优化后"
✅ 完成，对比数据

# 结果对比
[优化前] B站首页: 63.2s
[优化后] B站首页: 48.5s → 提升 23.3% ✨

[优化前] 视频页: 72.6s
[优化后] 视频页: 68.1s → 提升 6.2%

[优化前] 动态页: 55.8s
[优化后] 动态页: 51.2s → 提升 8.2%
```

## 🎯 总结

测试标记功能帮助你：
- ✅ 清晰区分不同版本的测试
- ✅ 方便对比优化效果
- ✅ 建立完整的性能演进历史
- ✅ 团队协作时保持一致性
- ✅ 快速定位特定测试结果

合理使用测试标记，让性能测试更加高效和有条理！🚀
